# -*- coding: utf-8 -*-
"""Build_Eval_Reddit(PRAW).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CE_VsyRe-v9mPi4-vbqH-jQZRYHwNmoP

**Stock Symbols**

This creates a list called nasdaq_100_symbols containing the stock symbols of the companies to be analyzed.

**Function: fetch_reddit_sentiment**

This function scrapes Reddit posts related to a specific stock_symbol from the "stocks" subreddit.
It uses TextBlob to analyze the sentiment of each post's title and content.
It returns the average sentiment score for the posts.

**Main Workflow:**

**1. Initialization:**

* start_date, end_date: These variables define the period for which stock data will be fetched.
The code iterates through each stock_symbol in the nasdaq_100_symbols list.

**2. Fetching Data:**

* **Reddit Sentiment:**
 * sentiment_score = fetch_reddit_sentiment(stock_symbol): Calls the fetch_reddit_sentiment function to scrape Reddit posts related to the current stock_symbol and calculate the average sentiment score.
* **Historical Stock Data:**
 * stock_data = fetch_stock_data(stock_symbol, start_date, end_date): Calls the fetch_stock_data function to download historical stock data for the current stock_symbol within the specified date range.

**3. Data Preparation:**

* **Sentiment Array:**
 * sentiment_array = np.full(stock_data.shape[0], sentiment_score): Creates an array filled with the calculated sentiment_score, having the same length as the stock data. This ensures that each stock data point is associated with the corresponding sentiment.
* **Preprocessing:**
 * X, y, scaler = preprocess_data(stock_data.values.reshape(-1, 1), sentiment_array): Calls the preprocess_data function to scale the stock data and sentiment scores, and create the input sequences (X) and target values (y) for the LSTM model. The scaler object is also returned for later use.

**4. Model Training and Evaluation:**

* **Data Splitting:**
 * train_size = int(len(X) * 0.8): Calculates the size of the training dataset (80% of the data).
 * X_train, X_test = X[:train_size], X[train_size:]: Splits the input data (X) into training and testing sets.
 * y_train, y_test = y[:train_size], y[train_size:]: Splits the target values (y) into training and testing sets.

* **Model Training:**
 * model = train_lstm_model(X_train, y_train): Calls the train_lstm_model function to build and train the LSTM model using the training data.

* **Model Evaluation:**
 * evaluate_model(model, X_test, y_test, scaler, stock_symbol): Calls the evaluate_model function to make predictions on the test data, reverse the scaling to get actual stock prices, and visualize the results (predicted vs. actual stock prices).
"""

import praw
import yfinance as yf
import pandas as pd
from textblob import TextBlob
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import matplotlib.pyplot as plt
import numpy as np

# Reddit API setup (replace with your credentials)
reddit = praw.Reddit(
    client_id="EzytCSjaTbNqdvTflEl4iw",
    client_secret="GIshSLj10RYsKTGobqmBzCL8hpc4ug",
    user_agent="Stock_News"
)

# List of NASDAQ-100 companies
nasdaq_100_symbols = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'TSLA', 'META']

# Function to scrape Reddit posts and extract sentiment
def fetch_reddit_sentiment(stock_symbol, limit=100):
    subreddit = reddit.subreddit("stocks")
    posts = subreddit.search(stock_symbol, limit=limit)
    sentiments = []
    for post in posts:
        blob = TextBlob(post.title + " " + post.selftext)
        sentiment = blob.sentiment.polarity
        sentiments.append(sentiment)
    return np.mean(sentiments) if sentiments else 0  # Average sentiment score

# Fetch historical stock data
def fetch_stock_data(symbols, start_date, end_date):
    data = yf.download(symbols, start=start_date, end=end_date)
    return data["Adj Close"]

# Preprocess data for LSTM
def preprocess_data(data, sentiment_scores, time_step=60):
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(data)
    sentiment_scores_scaled = MinMaxScaler((0, 1)).fit_transform(sentiment_scores.reshape(-1, 1))
    combined_data = np.hstack([scaled_data, sentiment_scores_scaled])

    X, y = [], []
    for i in range(len(combined_data) - time_step - 1):
        X.append(combined_data[i:(i + time_step)])
        y.append(combined_data[i + time_step, 0])  # Predict stock price
    return np.array(X), np.array(y), scaler

# Build and train LSTM model
def train_lstm_model(X_train, y_train, epochs=20, batch_size=32):
    model = Sequential([
        LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
        Dropout(0.2),
        LSTM(units=50, return_sequences=False),
        Dropout(0.2),
        Dense(units=1)
    ])
    model.compile(optimizer='adam', loss='mean_squared_error')
    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)
    return model

# Evaluate and plot predictions
def evaluate_model(model, X_test, y_test, scaler, stock_symbol):
    predictions = model.predict(X_test)
    predictions_rescaled = scaler.inverse_transform(predictions)
    y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))

    plt.figure(figsize=(10, 6))
    plt.plot(y_test_rescaled, label='Actual Prices')
    plt.plot(predictions_rescaled, label='Predicted Prices')
    plt.title(f'{stock_symbol} Stock Price Prediction')
    plt.xlabel('Time')
    plt.ylabel('Stock Price')
    plt.legend()
    plt.show()

# Main workflow
start_date = '2018-01-01'
end_date = '2023-01-01'

for stock_symbol in nasdaq_100_symbols:
    print(f"Processing {stock_symbol}...")

    # Fetch Reddit sentiment
    sentiment_score = fetch_reddit_sentiment(stock_symbol)
    print(f"Sentiment score for {stock_symbol}: {sentiment_score}")

    # Fetch historical stock data
    stock_data = fetch_stock_data(stock_symbol, start_date, end_date)

    # Create a sentiment array of the same length as the stock data
    sentiment_array = np.full(stock_data.shape[0], sentiment_score)

    # Preprocess data
    X, y, scaler = preprocess_data(stock_data.values.reshape(-1, 1), sentiment_array)

    # Split data into training and testing sets
    train_size = int(len(X) * 0.8)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    # Train LSTM model
    model = train_lstm_model(X_train, y_train)

    # Evaluate the model
    evaluate_model(model, X_test, y_test, scaler, stock_symbol)

"""**Main Workflow:**

Initialization:

start_date, end_date: Define the time period for fetching stock data.
The code iterates through each stock_symbol in the nasdaq_100_symbols list (e.g., 'AAPL', 'MSFT', etc.).

Data Collection:

Reddit Sentiment:

sentiment_score = fetch_reddit_sentiment(stock_symbol): Calls the fetch_reddit_sentiment function to get the average sentiment score for the current stock_symbol from Reddit posts.

Historical Stock Data:

stock_data = fetch_stock_data(stock_symbol, start_date, end_date): Calls the fetch_stock_data function to download historical stock prices (Adjusted Close) for the current stock_symbol.

Data Preparation:

Sentiment Array:
sentiment_array = np.full(stock_data.shape[0], sentiment_score): Creates an array with the same length as the stock data, where each element is the calculated sentiment_score. This associates the sentiment with each day's stock data.

Preprocessing:

X, y, scaler = preprocess_data(stock_data.values.reshape(-1, 1), sentiment_array):
Calls the preprocess_data function to:
Scale the stock prices and sentiment scores using MinMaxScaler.
Create input sequences (X) and target values (y) for the LSTM model. X contains historical data (stock prices and sentiment) used to predict y, which is the future stock price.
Returns the scaler object for later use (to reverse the scaling and get actual prices).

Model Training and Evaluation:

Data Splitting:

train_size = int(len(X) * 0.8): Calculates the size of the training dataset (80% of the data).
X_train, X_test = X[:train_size], X[train_size:]: Splits the input data (X) into training and testing sets.
y_train, y_test = y[:train_size], y[train_size:]: Splits the target values (y) into training and testing sets.

Model Training:

model = train_lstm_model(X_train, y_train): Calls the train_lstm_model function to build and train an LSTM neural network using the training data.

Model Evaluation:

evaluate_model_accuracy(model, X_test, y_test, scaler, stock_symbol):
Calls the evaluate_model_accuracy function to:
Make predictions on the test data
"""

import praw
import yfinance as yf
import pandas as pd
from textblob import TextBlob
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score

# Reddit API setup (replace with your credentials)
reddit = praw.Reddit(
    client_id="EzytCSjaTbNqdvTflEl4iw",
    client_secret="GIshSLj10RYsKTGobqmBzCL8hpc4ug",
    user_agent="Stock_News"
)

# List of NASDAQ-100 companies
nasdaq_100_symbols = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'TSLA', 'META']

# Function to scrape Reddit posts and extract sentiment
def fetch_reddit_sentiment(stock_symbol, limit=100):
    subreddit = reddit.subreddit("stocks")
    posts = subreddit.search(stock_symbol, limit=limit)
    sentiments = []
    for post in posts:
        blob = TextBlob(post.title + " " + post.selftext)
        sentiment = blob.sentiment.polarity
        sentiments.append(sentiment)
    return np.mean(sentiments) if sentiments else 0  # Average sentiment score

# Fetch historical stock data
def fetch_stock_data(symbols, start_date, end_date):
    data = yf.download(symbols, start=start_date, end=end_date)
    return data["Adj Close"]

# Preprocess data for LSTM
def preprocess_data(data, sentiment_scores, time_step=60):
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(data)
    sentiment_scores_scaled = MinMaxScaler((0, 1)).fit_transform(sentiment_scores.reshape(-1, 1))
    combined_data = np.hstack([scaled_data, sentiment_scores_scaled])

    X, y = [], []
    for i in range(len(combined_data) - time_step - 1):
        X.append(combined_data[i:(i + time_step)])
        y.append(combined_data[i + time_step, 0])  # Predict stock price
    return np.array(X), np.array(y), scaler

# Build and train LSTM model
def train_lstm_model(X_train, y_train, epochs=20, batch_size=32):
    model = Sequential([
        LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
        Dropout(0.2),
        LSTM(units=50, return_sequences=False),
        Dropout(0.2),
        Dense(units=1)
    ])
    model.compile(optimizer='adam', loss='mean_squared_error')
    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)
    return model

# Evaluate and print accuracy for each stock
def evaluate_model_accuracy(model, X_test, y_test, scaler, stock_symbol):
    # Make predictions
    predictions = model.predict(X_test)
    predictions_rescaled = scaler.inverse_transform(predictions)
    y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))

    # Calculate metrics
    mse = mean_squared_error(y_test_rescaled, predictions_rescaled)
    mape = mean_absolute_percentage_error(y_test_rescaled, predictions_rescaled)
    r2 = r2_score(y_test_rescaled, predictions_rescaled)

    print(f"Accuracy for {stock_symbol}:")
    print(f"Mean Squared Error (MSE): {mse}")
    print(f"Mean Absolute Percentage Error (MAPE): {mape * 100}%")
    print(f"R² Score: {r2}")



# Main workflow
start_date = '2018-01-01'
end_date = '2023-01-01'

for stock_symbol in nasdaq_100_symbols:
    print(f"\nProcessing {stock_symbol}...")

    # Fetch Reddit sentiment
    sentiment_score = fetch_reddit_sentiment(stock_symbol)
    print(f"Sentiment score for {stock_symbol}: {sentiment_score}")

    # Fetch historical stock data
    stock_data = fetch_stock_data(stock_symbol, start_date, end_date)

    # Create a sentiment array of the same length as the stock data
    sentiment_array = np.full(stock_data.shape[0], sentiment_score)

    # Preprocess data
    X, y, scaler = preprocess_data(stock_data.values.reshape(-1, 1), sentiment_array)

    # Split data into training and testing sets
    train_size = int(len(X) * 0.8)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    # Train LSTM model
    model = train_lstm_model(X_train, y_train)

    # Evaluate the model
    evaluate_model_accuracy(model, X_test, y_test, scaler, stock_symbol)