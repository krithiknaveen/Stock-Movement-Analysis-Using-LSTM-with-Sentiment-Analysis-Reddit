# -*- coding: utf-8 -*-
"""Sentiment_into_LTSM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jbz60eSmSuu3w-joz6URuyYsUL0JSJOG

***Integrating Sentiment Data into an LSTM Model for Stock Prediction***
"""

import numpy as np
import pandas as pd
import requests
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# RapidAPI credentials for Reddit Scraper
api_key = '4e0d6d9ce4msh3bc59dXXXXXXXX7ep170256jsn5653165127f2'
url = "https://reddit-scraper2.p.rapidapi.com/search_posts"
headers = {
    'x-rapidapi-host': "reddit-scraper2.p.rapidapi.com",
    'x-rapidapi-key': api_key
}

# Initialize sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

# Function to fetch Reddit posts from RapidAPI
def fetch_reddit_posts(stock_symbol, num_posts=10):
    querystring = {"query": stock_symbol, "limit": num_posts}
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        data = response.json()
        if "data" in data:
            return data["data"]  # Return the list of posts
        else:
            return []
    else:
        return []

# Function to analyze sentiment of Reddit posts
def analyze_sentiment_from_reddit(stock_symbol, num_posts=10):
    posts = fetch_reddit_posts(stock_symbol, num_posts)
    if not posts:
        return 0  # No posts, return neutral sentiment

    sentiments = []
    for post in posts:
        title = post.get('title', '')
        sentiment_score = analyzer.polarity_scores(title)['compound']
        sentiments.append(sentiment_score)

    return sum(sentiments) / len(sentiments) if sentiments else 0

# Function to fetch historical stock data (Yahoo Finance)
def fetch_stock_data(stock_symbol, start_date='2022-01-01', end_date='2024-01-01'):
    import yfinance as yf
    stock_data = yf.download(stock_symbol, start=start_date, end=end_date)
    return stock_data[['Open', 'High', 'Low', 'Close', 'Volume']]

# Function to prepare data for LSTM model
def prepare_data_for_lstm(stock_symbol, num_posts=10):
    # Fetch stock price data
    stock_data = fetch_stock_data(stock_symbol)

    # Add sentiment data
    stock_data['Sentiment'] = stock_data.index.map(lambda x: analyze_sentiment_from_reddit(stock_symbol, num_posts))

    # Drop missing values (if any)
    stock_data.dropna(inplace=True)

    # Normalize the data
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(stock_data[['Open', 'High', 'Low', 'Close', 'Volume', 'Sentiment']])

    # Prepare sequences for LSTM
    X, y = [], []
    for i in range(60, len(scaled_data)):
        X.append(scaled_data[i-60:i])  # Last 60 days of data
        y.append(scaled_data[i, 3])    # Predict the 'Close' price

    X = np.array(X)
    y = np.array(y)

    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

    return X_train, X_test, y_train, y_test, scaler

# Build and compile LSTM model
def build_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))
    model.add(LSTM(units=50, return_sequences=False))
    model.add(Dense(units=1))  # Predicting the 'Close' price
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Train the model
def train_and_evaluate_model(stock_symbol, num_posts=10):
    # Prepare the data
    X_train, X_test, y_train, y_test, scaler = prepare_data_for_lstm(stock_symbol, num_posts)

    # Build the model
    model = build_lstm_model(X_train.shape[1:])

    # Train the model
    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

    # Evaluate the model
    test_loss = model.evaluate(X_test, y_test)
    print(f"Test Loss (MSE): {test_loss}")

    return model, scaler

# Example usage for AAPL (Apple Inc.)
model, scaler = train_and_evaluate_model('AAPL', num_posts=10)

"""Integrating Sentiment Data into an LSTM Model for Stock Prediction for GOOGLE"""

api_key = '4c83acc98msh12cfe78e2XXXXXXXXXXXXXXXXX2299bf60d36'
url = "https://reddit-scraper2.p.rapidapi.com/search_posts"

headers = {
    'x-rapidapi-host': "reddit-scraper2.p.rapidapi.com",
    'x-rapidapi-key': api_key
}

# Initialize sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

# Function to fetch Reddit posts from RapidAPI
def fetch_reddit_posts(stock_symbol, num_posts=10):
    querystring = {"query": stock_symbol, "limit": num_posts}
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        data = response.json()
        if "data" in data:
            return data["data"]  # Return the list of posts
        else:
            return []
    else:
        return []

# Function to analyze sentiment of Reddit posts
def analyze_sentiment_from_reddit(stock_symbol, num_posts=10):
    posts = fetch_reddit_posts(stock_symbol, num_posts)
    if not posts:
        return 0  # No posts, return neutral sentiment

    sentiments = []
    for post in posts:
        title = post.get('title', '')
        sentiment_score = analyzer.polarity_scores(title)['compound']
        sentiments.append(sentiment_score)

    return sum(sentiments) / len(sentiments) if sentiments else 0

# Function to fetch historical stock data (Yahoo Finance)
def fetch_stock_data(stock_symbol, start_date='2022-01-01', end_date='2024-01-01'):
    stock_data = yf.download(stock_symbol, start=start_date, end=end_date)
    return stock_data[['Open', 'High', 'Low', 'Close', 'Volume']]

# Adding Technical Indicators (e.g., SMA, RSI, EMA)
def add_technical_indicators(df):
    # Ensure 'Close' is a 1-dimensional array
    close_prices = df['Close'].to_numpy().flatten()  # Convert to float and ensure 1D

    # Use talib functions directly instead of ta.trend
    df['SMA'] = ta.SMA(close_prices, timeperiod=14)
    df['EMA'] = ta.EMA(close_prices, timeperiod=14)
    df['RSI'] = ta.RSI(close_prices, timeperiod=14)
    return df

# Function to prepare data for LSTM model
def prepare_data_for_lstm(stock_symbol, num_posts=10):
    # Fetch stock price data
    stock_data = fetch_stock_data(stock_symbol)

    # Add sentiment data
    stock_data['Sentiment'] = stock_data.index.map(lambda x: analyze_sentiment_from_reddit(stock_symbol, num_posts))

    # Add technical indicators
    stock_data = add_technical_indicators(stock_data)

    # Drop missing values (if any)
    stock_data.dropna(inplace=True)

    # Normalize the data
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(stock_data[['Open', 'High', 'Low', 'Close', 'Volume', 'Sentiment', 'SMA', 'EMA', 'RSI']])

    # Prepare sequences for LSTM
    X, y = [], []
    for i in range(60, len(scaled_data)):
        X.append(scaled_data[i-60:i])  # Last 60 days of data
        y.append(scaled_data[i, 3])    # Predict the 'Close' price

    X = np.array(X)
    y = np.array(y)

    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

    return X_train, X_test, y_train, y_test, scaler

# Build and compile LSTM model
def build_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(units=100, return_sequences=True, input_shape=input_shape))
    model.add(LSTM(units=100, return_sequences=False))
    model.add(Dense(units=1))  # Predicting the 'Close' price
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Train the model
def train_and_evaluate_model(stock_symbol, num_posts=10):
    # Prepare the data
    X_train, X_test, y_train, y_test, scaler = prepare_data_for_lstm(stock_symbol, num_posts)

    # Build the model
    model = build_lstm_model(X_train.shape[1:])

    # Train the model
    model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

    # Evaluate the model
    test_loss = model.evaluate(X_test, y_test)
    print(f"Test Loss (MSE): {test_loss}")

    return model, scaler

# Visualize predictions vs. actual stock prices
def visualize_predictions(stock_symbol, model, scaler, num_posts=10):
    # Fetch stock data for visualization
    stock_data = fetch_stock_data(stock_symbol)
    stock_data['Sentiment'] = stock_data.index.map(lambda x: analyze_sentiment_from_reddit(stock_symbol, num_posts))
    stock_data = add_technical_indicators(stock_data)
    stock_data.dropna(inplace=True)

    # Normalize the data for prediction
    scaled_data = scaler.transform(stock_data[['Open', 'High', 'Low', 'Close', 'Volume', 'Sentiment', 'SMA', 'EMA', 'RSI']])

    # Prepare test data for prediction
    X = []
    for i in range(60, len(scaled_data)):
        X.append(scaled_data[i-60:i])

    X = np.array(X)
    predictions = model.predict(X)

    # Inverse scaling to get actual prices
    predictions = scaler.inverse_transform(np.concatenate([np.zeros((predictions.shape[0], X.shape[2]-1)), predictions], axis=1))[:, -1]
    actual = stock_data['Close'][60:].values

    # Plotting
    plt.figure(figsize=(10,6))
    plt.plot(actual, color='blue', label='Actual Stock Price')
    plt.plot(predictions, color='red', label='Predicted Stock Price')
    plt.title(f'{stock_symbol} Stock Price Prediction')
    plt.xlabel('Time')
    plt.ylabel('Stock Price')
    plt.legend()
    plt.show()

# Example usage for AAPL (Apple Inc.)
model, scaler = train_and_evaluate_model('GOOG', num_posts=10)
visualize_predictions('GOOG', model, scaler)